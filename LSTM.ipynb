{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras.callbacks\n",
    "from music21 import *\n",
    "\n",
    "import re\n",
    "import xml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dense, Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('environment.txt') as f:\n",
    "    env = f.readlines()\n",
    "# you may also want to remove whitespace characters like \\n at the end of each line\n",
    "env = [x.strip() for x in env]\n",
    "\n",
    "us = environment.UserSettings()\n",
    "us['musicxmlPath'] = env[0]\n",
    "us['musescoreDirectPNGPath'] = env[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bachRegex = re.compile(r'.*bwv.*', re.IGNORECASE)\n",
    "bd = corpus.search(bachRegex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertSongToDataFrame(song):\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    currentKeySignature = None\n",
    "    currentTimeSignature = None\n",
    "    for measure in song.measures(0, None, collect=('TimeSignature','KeySignature')):\n",
    "        if measure.keySignature:\n",
    "            currentKeySignature = measure.keySignature\n",
    "        if measure.timeSignature:\n",
    "            currentTimeSignature = measure.timeSignature.ratioString\n",
    "        for i in measure.iter:\n",
    "            if type(i) is chord.Chord:\n",
    "                df2 = pd.DataFrame([currentKeySignature, currentTimeSignature, roman.romanNumeralFromChord(i, currentKeySignature).figure, i.duration.quarterLength, i.offset + measure.offset])\n",
    "                df = df.append(df2[0], ignore_index = True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-14-63e9a70184f8>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-14-63e9a70184f8>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    only run if you want to create csv files\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# only run if you want to create csv files\n",
    "import os.path\n",
    "import os\n",
    "if not os.path.exists('./data'):\n",
    "    os.makedirs(\"./data\")\n",
    "for s in bd:\n",
    "    fname = \"./data/\" + s.sourcePath.split(\"/\")[1]+'.csv'\n",
    "    if os.path.isfile(fname):\n",
    "        continue\n",
    "    df = convertSongToDataFrame(s.parse().chordify())\n",
    "    df.to_csv(fname, sep=',')\n",
    "    \n",
    "#failed at bwv276.mxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0  1    2     3       4\n",
      "0    7  4   99  1.00    0.00\n",
      "1    7  4  329  0.50    1.00\n",
      "2    7  4  329  0.50    1.50\n",
      "3    7  4   99  0.50    2.00\n",
      "4    7  4   99  0.50    2.50\n",
      "5    7  4  774  0.50    3.00\n",
      "6    7  4  774  0.50    3.50\n",
      "7    7  4  122  0.50    4.00\n",
      "8    7  4  122  0.50    4.50\n",
      "9    7  4  237  0.25    5.00\n",
      "10   7  4  237  0.25    5.25\n",
      "11   7  4  291  0.25    5.50\n",
      "12   7  4  599  0.25    5.75\n",
      "13   7  4  262  0.50    6.00\n",
      "14   7  4  851  0.50    6.50\n",
      "15   7  4   99  1.00    7.00\n",
      "16   7  4   99  1.00    8.00\n",
      "17   7  4  237  0.50    9.00\n",
      "18   7  4  583  0.25    9.50\n",
      "19   7  4  273  0.25    9.75\n",
      "20   7  4  895  0.50   10.00\n",
      "21   7  4  321  0.25   10.50\n",
      "22   7  4  886  0.25   10.75\n",
      "23   7  4  122  0.50   11.00\n",
      "24   7  4  163  0.50   11.50\n",
      "25   7  4  305  0.50   12.00\n",
      "26   7  4  305  0.50   12.50\n",
      "27   7  4  813  0.50   13.00\n",
      "28   7  4   47  0.25   13.50\n",
      "29   7  4  832  0.25   13.75\n",
      "..  .. ..  ...   ...     ...\n",
      "193  3  4  441  0.25  102.75\n",
      "194  3  4  401  0.50  103.00\n",
      "195  3  4  540  0.25  103.50\n",
      "196  3  4  749  0.25  103.75\n",
      "197  3  4  370  0.50  104.00\n",
      "198  3  4  250  0.25  104.50\n",
      "199  3  4  372  0.25  104.75\n",
      "200  3  4  550  0.50  105.00\n",
      "201  3  4  452  0.25  105.50\n",
      "202  3  4  589  0.25  105.75\n",
      "203  3  4  211  0.50  106.00\n",
      "204  3  4  808  0.50  106.50\n",
      "205  3  4  862  0.50  107.00\n",
      "206  3  4   90  0.50  107.50\n",
      "207  3  4  364  2.00  108.00\n",
      "208  3  4  434  0.50  110.00\n",
      "209  3  4  757  0.50  110.50\n",
      "210  3  4  805  0.50  111.00\n",
      "211  3  4  163  0.50  111.50\n",
      "212  3  4  743  0.50  112.00\n",
      "213  3  4  757  0.25  112.50\n",
      "214  3  4  174  0.25  112.75\n",
      "215  3  4  745  0.25  113.00\n",
      "216  3  4  773  0.25  113.25\n",
      "217  3  4  716  0.50  113.50\n",
      "218  3  4  749  0.50  114.00\n",
      "219  3  4  151  0.50  114.50\n",
      "220  3  4  749  0.50  115.00\n",
      "221  3  4  618  0.50  115.50\n",
      "222  3  4  151  2.00  116.00\n",
      "\n",
      "[17685 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scores = []\n",
    "import glob, os\n",
    "os.chdir(\"C:/Users/errpo/Desktop/projects/project folder/music_prediction/data\")\n",
    "for file in glob.glob(\"*.csv\"):\n",
    "    scores.append(file)\n",
    "scoreDf = []\n",
    "\n",
    "for i in scores:\n",
    "    score = pd.read_csv(i,usecols=[1,2,3,4,5])\n",
    "    scoreDf.append(score)\n",
    "    \n",
    "# print(scoreDf[1])\n",
    "scores = pd.concat(scoreDf)\n",
    "\n",
    "keySignatures = pd.DataFrame([scores['0'].astype('category').cat.codes.unique()], columns=scores['0'].unique())\n",
    "timeSignatures = pd.DataFrame([scores['1'].astype('category').cat.codes.unique()], columns=scores['1'].unique())\n",
    "chords = pd.DataFrame([scores['2'].astype('category').cat.codes.unique()], columns=scores['2'].unique())\n",
    "\n",
    "scores['0'] = scores['0'].astype('category').cat.codes\n",
    "scores['1'] = scores['1'].astype('category').cat.codes\n",
    "scores['2'] = scores['2'].astype('category').cat.codes\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_csv('bwv1.6.mxl.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scores.values\n",
    "examples = 100\n",
    "y_examples = 10\n",
    "nb_samples = len(data) - examples - y_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_list = [np.expand_dims(np.atleast_2d(data[i:examples+i,:]), axis=0) for i in range(nb_samples)]\n",
    "input_mat = np.concatenate(input_list, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list = [np.atleast_2d(data[i+examples:examples+i+y_examples,0]) for i in range(nb_samples)]\n",
    "target_mat = np.concatenate(target_list, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = input_mat.shape[0]\n",
    "features = input_mat.shape[2]\n",
    "hidden = 64\n",
    "model = Sequential()\n",
    "model.add(LSTM(hidden, input_shape=(examples, features)))\n",
    "model.add(Dropout(.2))\n",
    "# model.add(Flatten())\n",
    "model.add(Dense(y_examples))\n",
    "model.add(Activation('linear'))\n",
    "model.compile(loss='mse', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\errpo\\anaconda3\\envs\\music\\lib\\site-packages\\keras\\models.py:939: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "17575/17575 [==============================] - 26s 1ms/step - loss: 49.8768\n",
      "Epoch 2/100\n",
      "17575/17575 [==============================] - 26s 1ms/step - loss: 25.3969\n",
      "Epoch 3/100\n",
      "17575/17575 [==============================] - 22s 1ms/step - loss: 16.6499\n",
      "Epoch 4/100\n",
      "17575/17575 [==============================] - 20s 1ms/step - loss: 11.8401\n",
      "Epoch 5/100\n",
      "17575/17575 [==============================] - 19s 1ms/step - loss: 7.6324\n",
      "Epoch 6/100\n",
      "17575/17575 [==============================] - 18s 1ms/step - loss: 5.6216\n",
      "Epoch 7/100\n",
      "17575/17575 [==============================] - 18s 1ms/step - loss: 4.5575\n",
      "Epoch 8/100\n",
      "17575/17575 [==============================] - 20s 1ms/step - loss: 3.9406\n",
      "Epoch 9/100\n",
      "17575/17575 [==============================] - 18s 1ms/step - loss: 3.5756\n",
      "Epoch 10/100\n",
      "17575/17575 [==============================] - 26s 1ms/step - loss: 3.3802\n",
      "Epoch 11/100\n",
      "17575/17575 [==============================] - 21s 1ms/step - loss: 3.1915\n",
      "Epoch 12/100\n",
      "17575/17575 [==============================] - 17s 995us/step - loss: 3.0583\n",
      "Epoch 13/100\n",
      "17575/17575 [==============================] - 17s 974us/step - loss: 2.9637\n",
      "Epoch 14/100\n",
      "17575/17575 [==============================] - 19s 1ms/step - loss: 2.8708\n",
      "Epoch 15/100\n",
      "17575/17575 [==============================] - 20s 1ms/step - loss: 2.8773\n",
      "Epoch 16/100\n",
      "17575/17575 [==============================] - 20s 1ms/step - loss: 2.7743\n",
      "Epoch 17/100\n",
      "17575/17575 [==============================] - 19s 1ms/step - loss: 2.6777\n",
      "Epoch 18/100\n",
      "17575/17575 [==============================] - 20s 1ms/step - loss: 2.6556\n",
      "Epoch 19/100\n",
      "17575/17575 [==============================] - 21s 1ms/step - loss: 2.6706\n",
      "Epoch 20/100\n",
      "17575/17575 [==============================] - 22s 1ms/step - loss: 2.5656\n",
      "Epoch 21/100\n",
      "17575/17575 [==============================] - 19s 1ms/step - loss: 2.5736\n",
      "Epoch 22/100\n",
      "17575/17575 [==============================] - 18s 1ms/step - loss: 2.4820\n",
      "Epoch 23/100\n",
      "17575/17575 [==============================] - 21s 1ms/step - loss: 2.4826\n",
      "Epoch 24/100\n",
      "17575/17575 [==============================] - 21s 1ms/step - loss: 2.4746\n",
      "Epoch 25/100\n",
      "17575/17575 [==============================] - 21s 1ms/step - loss: 2.3953\n",
      "Epoch 26/100\n",
      "17575/17575 [==============================] - 20s 1ms/step - loss: 2.3781\n",
      "Epoch 27/100\n",
      "17575/17575 [==============================] - 21s 1ms/step - loss: 2.3687\n",
      "Epoch 28/100\n",
      "17575/17575 [==============================] - 23s 1ms/step - loss: 2.3215\n",
      "Epoch 29/100\n",
      "17575/17575 [==============================] - 23s 1ms/step - loss: 2.3252\n",
      "Epoch 30/100\n",
      "17575/17575 [==============================] - 22s 1ms/step - loss: 2.3274\n",
      "Epoch 31/100\n",
      "17575/17575 [==============================] - 20s 1ms/step - loss: 2.3138\n",
      "Epoch 32/100\n",
      "17575/17575 [==============================] - 24s 1ms/step - loss: 2.2517\n",
      "Epoch 33/100\n",
      "17575/17575 [==============================] - 22s 1ms/step - loss: 2.2568\n",
      "Epoch 34/100\n",
      "17575/17575 [==============================] - 24s 1ms/step - loss: 2.2274\n",
      "Epoch 35/100\n",
      "17575/17575 [==============================] - 26s 1ms/step - loss: 2.2097\n",
      "Epoch 36/100\n",
      "17575/17575 [==============================] - 21s 1ms/step - loss: 2.2000\n",
      "Epoch 37/100\n",
      "17575/17575 [==============================] - 23s 1ms/step - loss: 2.1918\n",
      "Epoch 38/100\n",
      "17575/17575 [==============================] - 22s 1ms/step - loss: 2.2042\n",
      "Epoch 39/100\n",
      "17575/17575 [==============================] - 33s 2ms/step - loss: 2.1530\n",
      "Epoch 40/100\n",
      "17575/17575 [==============================] - 20s 1ms/step - loss: 2.1887\n",
      "Epoch 41/100\n",
      "17575/17575 [==============================] - 22s 1ms/step - loss: 2.1490\n",
      "Epoch 42/100\n",
      "17575/17575 [==============================] - 24s 1ms/step - loss: 2.0929\n",
      "Epoch 43/100\n",
      "17575/17575 [==============================] - 24s 1ms/step - loss: 2.1159\n",
      "Epoch 44/100\n",
      "17575/17575 [==============================] - 19s 1ms/step - loss: 2.1165\n",
      "Epoch 45/100\n",
      "17575/17575 [==============================] - 21s 1ms/step - loss: 2.0958\n",
      "Epoch 46/100\n",
      "17575/17575 [==============================] - 20s 1ms/step - loss: 2.0974\n",
      "Epoch 47/100\n",
      "17575/17575 [==============================] - 22s 1ms/step - loss: 2.0884\n",
      "Epoch 48/100\n",
      "17575/17575 [==============================] - 24s 1ms/step - loss: 2.0769\n",
      "Epoch 49/100\n",
      "17575/17575 [==============================] - 19s 1ms/step - loss: 2.0758\n",
      "Epoch 50/100\n",
      "17575/17575 [==============================] - 21s 1ms/step - loss: 2.0302\n",
      "Epoch 51/100\n",
      "17575/17575 [==============================] - 23s 1ms/step - loss: 2.0462\n",
      "Epoch 52/100\n",
      "17575/17575 [==============================] - 22s 1ms/step - loss: 2.0297\n",
      "Epoch 53/100\n",
      "17575/17575 [==============================] - 21s 1ms/step - loss: 2.0052\n",
      "Epoch 54/100\n",
      "17575/17575 [==============================] - 23s 1ms/step - loss: 2.0633\n",
      "Epoch 55/100\n",
      "17575/17575 [==============================] - 21s 1ms/step - loss: 1.9773\n",
      "Epoch 56/100\n",
      "17575/17575 [==============================] - 17s 978us/step - loss: 1.9879\n",
      "Epoch 57/100\n",
      "17575/17575 [==============================] - 17s 987us/step - loss: 1.9788\n",
      "Epoch 58/100\n",
      "17575/17575 [==============================] - 18s 1ms/step - loss: 1.9636\n",
      "Epoch 59/100\n",
      "17575/17575 [==============================] - 19s 1ms/step - loss: 1.9558\n",
      "Epoch 60/100\n",
      "17575/17575 [==============================] - 19s 1ms/step - loss: 1.9520\n",
      "Epoch 61/100\n",
      "17575/17575 [==============================] - 21s 1ms/step - loss: 1.9338\n",
      "Epoch 62/100\n",
      "17575/17575 [==============================] - 21s 1ms/step - loss: 1.9718\n",
      "Epoch 63/100\n",
      "17575/17575 [==============================] - 20s 1ms/step - loss: 1.8954\n",
      "Epoch 64/100\n",
      "17575/17575 [==============================] - 19s 1ms/step - loss: 1.9109\n",
      "Epoch 65/100\n",
      "17575/17575 [==============================] - 17s 993us/step - loss: 1.8838\n",
      "Epoch 66/100\n",
      "17575/17575 [==============================] - 19s 1ms/step - loss: 1.9199\n",
      "Epoch 67/100\n",
      "17575/17575 [==============================] - 20s 1ms/step - loss: 1.8982\n",
      "Epoch 68/100\n",
      "17575/17575 [==============================] - 19s 1ms/step - loss: 1.9005\n",
      "Epoch 69/100\n",
      "17575/17575 [==============================] - 19s 1ms/step - loss: 1.9055\n",
      "Epoch 70/100\n",
      "17575/17575 [==============================] - 20s 1ms/step - loss: 1.8731\n",
      "Epoch 71/100\n",
      "17575/17575 [==============================] - 20s 1ms/step - loss: 1.9059\n",
      "Epoch 72/100\n",
      "17575/17575 [==============================] - 19s 1ms/step - loss: 1.8488\n",
      "Epoch 73/100\n",
      "17575/17575 [==============================] - 21s 1ms/step - loss: 1.8801\n",
      "Epoch 74/100\n",
      "17575/17575 [==============================] - 19s 1ms/step - loss: 1.8680\n",
      "Epoch 75/100\n",
      "17575/17575 [==============================] - 20s 1ms/step - loss: 1.8615\n",
      "Epoch 76/100\n",
      "17575/17575 [==============================] - 20s 1ms/step - loss: 1.8358\n",
      "Epoch 77/100\n",
      "17575/17575 [==============================] - 19s 1ms/step - loss: 1.8496\n",
      "Epoch 78/100\n",
      "17575/17575 [==============================] - 20s 1ms/step - loss: 1.8442\n",
      "Epoch 79/100\n",
      "17575/17575 [==============================] - 19s 1ms/step - loss: 1.8019\n",
      "Epoch 80/100\n",
      "17575/17575 [==============================] - 19s 1ms/step - loss: 1.8673\n",
      "Epoch 81/100\n",
      "17575/17575 [==============================] - 20s 1ms/step - loss: 1.7882\n",
      "Epoch 82/100\n",
      "17575/17575 [==============================] - 21s 1ms/step - loss: 1.7896\n",
      "Epoch 83/100\n",
      "17575/17575 [==============================] - 19s 1ms/step - loss: 1.8099\n",
      "Epoch 84/100\n",
      "17575/17575 [==============================] - 18s 1ms/step - loss: 1.7749\n",
      "Epoch 85/100\n",
      "17575/17575 [==============================] - 16s 924us/step - loss: 1.7683\n",
      "Epoch 86/100\n",
      "17575/17575 [==============================] - 17s 946us/step - loss: 1.8045\n",
      "Epoch 87/100\n",
      "17575/17575 [==============================] - 16s 931us/step - loss: 1.7759\n",
      "Epoch 88/100\n",
      "17575/17575 [==============================] - 17s 969us/step - loss: 1.7922\n",
      "Epoch 89/100\n",
      "17575/17575 [==============================] - 20s 1ms/step - loss: 1.7896\n",
      "Epoch 90/100\n",
      "17575/17575 [==============================] - 19s 1ms/step - loss: 1.7720\n",
      "Epoch 91/100\n",
      "17575/17575 [==============================] - 18s 1ms/step - loss: 1.7719\n",
      "Epoch 92/100\n",
      "17575/17575 [==============================] - 18s 1ms/step - loss: 1.7682\n",
      "Epoch 93/100\n",
      "17575/17575 [==============================] - 18s 1ms/step - loss: 1.7733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/100\n",
      "17575/17575 [==============================] - 18s 1ms/step - loss: 1.7300\n",
      "Epoch 95/100\n",
      "17575/17575 [==============================] - 21s 1ms/step - loss: 1.7349\n",
      "Epoch 96/100\n",
      "17575/17575 [==============================] - 18s 1ms/step - loss: 1.7580\n",
      "Epoch 97/100\n",
      "17575/17575 [==============================] - 19s 1ms/step - loss: 1.7461\n",
      "Epoch 98/100\n",
      "17575/17575 [==============================] - 19s 1ms/step - loss: 1.7401\n",
      "Epoch 99/100\n",
      "17575/17575 [==============================] - 19s 1ms/step - loss: 1.6909\n",
      "Epoch 100/100\n",
      "17575/17575 [==============================] - 19s 1ms/step - loss: 1.7195\n"
     ]
    }
   ],
   "source": [
    "history = LossHistory()\n",
    "model = model.fit(input_mat, target_mat, nb_epoch=100, batch_size=400, callbacks=[history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'History' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-752e6661b986>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainPredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'History' object has no attribute 'predict'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
